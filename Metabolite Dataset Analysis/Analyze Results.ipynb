{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Load dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "from IPython.display import SVG\n",
        "from graphviz import Source\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data  \u003d pd.read_csv(\u0027results7parsed.csv\u0027)\n",
        "\n",
        "# sort by best CA\n",
        "avg_ca \u003d data.sort_values(\u0027Average CA\u0027, ascending\u003dFalse)\n",
        "avg_rfw \u003d data.sort_values(\u0027RFW CA\u0027, ascending\u003dFalse)\n",
        "avg_dt \u003d data.sort_values(\u0027DT CA\u0027, ascending\u003dFalse)\n",
        "\n",
        "# get best 5 average features\n",
        "with open(\u0027CA Features.csv\u0027, \u0027w\u0027)as f :\n",
        "    f.write(\u0027Avg\u0027)\n",
        "    f.write(avg_ca[:5].to_csv())\n",
        "    \n",
        "# get best 5 random forest features\n",
        "with open(\u0027CA Features.csv\u0027, \u0027a\u0027)as f :\n",
        "    f.write(\u0027Rfw\u0027)\n",
        "    f.write(avg_rfw[:5].to_csv())\n",
        "    \n",
        "# get 5 best decision tree features \n",
        "with open(\u0027CA Features.csv\u0027, \u0027a\u0027)as f :\n",
        "    f.write(\u0027Dtree\u0027)\n",
        "    f.write(avg_dt[:5].to_csv())\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now, let us take the most interesting features from the csv and write a parser to analyze the feature importances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "\ns \u003d\u0027\u0027\u0027nof_OH, nof_NH2, nof_SO3H, C_R0, nof_HBA, PSA/Area, molLogS\nnof_OH, posCharge/Volume, C_R0, nof_HBA, PSA/Area, molLogS, molLogP\nnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_HBA, PSA/Area\nnof_OH, nof_NH2, nof_PO4, C_R0, nof_posCharge, nof_HBA, PSA/Area\nnof_OH, nof_NH2, nof_SO3H, negCharge/Volume, nof_HBA, PSA/Area, molLogS\nnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_negCharge, PSA/Area\nnof_SO3H, posCharge/Volume, C_R1, nof_posCharge, nof_HBA, PSA/Area, molLogS\nnof_OH, nof_SO3H, nof_negCharge, nof_posCharge, PSA/Area, molPSA, molLogP\nnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_posCharge, PSA/Area\nnof_OH, nof_NH2, nof_SO3H, C_R0, nof_posCharge, nof_HBA, molPSA\nnof_OH, nof_NH2, nof_SH, C_R0, nof_posCharge, nof_HBA, molPSA\nnof_SO3H, C_sp3, C_R0, C_R2, nof_HBA, molLogS, molLogP\nnof_OH, nof_NH2, nof_PO4, posCharge/Volume, C_R0, nof_HBA, molLogS\nnof_OH, nof_NH2, C_sp3, C_R0, nof_posCharge, nof_HBA, molLogS\nnof_OH, nof_NH2, nof_SO3H, C_R0, C_R2, nof_HBA, molVolume\nnof_OH, nof_NH2, nof_PO4, C_sp3, nof_HBA, PSA/Area, molLogS\nnof_SH, nof_PO4, posCharge/Volume, C_sp3, nof_posCharge, nof_HBA, PSA/Area\nnof_acetyl, nof_SO3H, C_R0, nof_HBA, PSA/Area, molLogS, molLogP\nnof_acetyl, nof_NH2, posCharge/Volume, C_sp3, C_R2, PSA/Area, molLogS\nnof_OH, nof_NH2, posCharge/Volume, nof_negCharge, nof_HBA, PSA/Area, molLogS\nnof_OH, nof_NH2, nof_PO4, C_R0, nof_posCharge, nof_HBA, molLogP\nnof_OH, nof_NH2, nof_SH, C_R0, nof_HBA, PSA/Area, molLogS\nnof_acetyl, nof_OH, nof_NH2, C_sp3, nof_HBA, PSA/Area, molLogS\nPSA/Area, nof_Rings, Complexity, nof_SO3H, nof_OH, nof_Chirals, C_R0\u0027\u0027\u0027\n\nfeatures \u003d  [sorted([x]) for x in s.split(\u0027\\n\u0027)]\nprint(features)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Project Settings\n",
        "\n",
        "Specified here are the paths for the data and the features to run over in the list of best features.\n",
        "Each entry in the list is a list containing one single string of the features to try, comma seperated. In this way it is easy to write a script to \n",
        "add entries to try very easily. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "##### set hyperparams\n",
        "NUM_ITER \u003d 10 # number of times to run 10foldxval to get a statistical degree of confidence\n",
        "\n",
        "\u0027\u0027\u0027 HYPERPARAMS FOR DECISION TREE\n",
        " \n",
        " These parameters implement a rudimentary pruning algorithm, would ideally like to use AB pruning\u0027\u0027\u0027\n",
        "enable_pruning \u003d True\n",
        "# maximum depth of dtree\n",
        "max_depth \u003d 5\n",
        "# how many samples your need atleast, at a LEAF node\n",
        "min_samples \u003d 3\n",
        "\n",
        "##### set parameters\n",
        "path_train_data \u003d \u0027train.csv\u0027\n",
        "path_test_data \u003d \u0027test.csv\u0027\n",
        "path_all_data \u003d \u0027Dataset Correlated Removed.csv\u0027\n",
        "\n",
        "# set features here\n",
        "\n",
        "best_features \u003d features\n",
        "\n",
        "best_features \u003d [list(map(str.strip, x[0].split(\u0027,\u0027))) for x in best_features]\n",
        "\n",
        "k \u003d len(best_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "This code loads dataset into the variables below and converts the labels to categorical 0, 1 pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "all_data \u003d pd.DataFrame(pd.read_csv(path_all_data))\n",
        "all_labels \u003d all_data[\u0027SLC\u0027].astype(\u0027category\u0027).cat.codes\n",
        "# drop labels\n",
        "all_data.drop(\u0027SLC\u0027, axis\u003d1, inplace\u003dTrue)\n",
        "\n",
        "train_data \u003d pd.DataFrame(pd.read_csv(path_train_data))\n",
        "train_labels \u003d train_data[\u0027SLC\u0027].astype(\u0027category\u0027).cat.codes\n",
        "# drop labels\n",
        "\n",
        "train_data.drop(\u0027SLC\u0027, axis\u003d1, inplace\u003dTrue)\n",
        "\n",
        "test_data \u003d pd.DataFrame(pd.read_csv(path_test_data))\n",
        "test_labels \u003d test_data[\u0027SLC\u0027].astype(\u0027category\u0027).cat.codes\n",
        "# drop labels\n",
        "test_data.drop(\u0027SLC\u0027, axis\u003d1, inplace\u003dTrue)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## AUC and Classification Accuracy - Decision Tree\n",
        "\n",
        "The code below will find the classification accuracy using 10-fold cross-validation using stratified sampling to help class imbalance. The AUC on the test split is also found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "# visualize decision tree for input features\n\nd_trees \u003d []\n\n\n# find CA - uses 10-fold cross validation \n# with stratified sampling to help with class imbalance\n# and simple average over subsets\ndt_cas \u003d []\n\n# maintain list of cas over a period\ndt_ca_matrix \u003d []\n\n# run the thing NUM_ITER times\nfor _ in range(NUM_ITER):\n    for i in range(k):\n        aucs \u003d []\n        # make fold\n        skf \u003d StratifiedKFold(n_splits\u003d10, shuffle\u003dTrue)\n        for trx, tex in skf.split(all_data, all_labels):\n            # strip data to required features\n            subset_data \u003d all_data.filter(best_features[i], axis\u003d1)\n            \n            # find auc\n            dtree \u003d DecisionTreeClassifier(presort\u003dTrue, max_depth\u003dmax_depth, min_samples_leaf\u003dmin_samples)\n            dtree.fit(subset_data.iloc[trx, :], all_labels.iloc[trx])        \n            pred \u003d dtree.predict(subset_data.iloc[tex, :])\n            labels \u003d all_labels.iloc[tex]\n            \n            acc \u003d roc_auc_score(labels, pred)\n            # record auc to average later\n            aucs.append(acc)\n        \n        dt_cas.append(np.mean(aucs))\n        \n    dt_ca_matrix.append(list(dt_cas))\n    dt_cas.clear()\n\n\ndt_ca_matrix \u003d np.array(dt_ca_matrix)\ndt_cas \u003d dt_ca_matrix.mean(axis\u003d0)\ndt_cas_std \u003d dt_ca_matrix.std(axis\u003d0)\n\n# find AUC \ndt_aucs \u003d []\n\n# run k-fold validation\nfor i in range(k):\n    subset_test_data \u003d test_data.filter(best_features[i], axis\u003d1)\n    subset_train_data \u003d train_data.filter(best_features[i], axis\u003d1)\n    \n    clf \u003d DecisionTreeClassifier(presort\u003dTrue, max_depth\u003dmax_depth, min_samples_leaf\u003dmin_samples)\n    clf.fit(subset_train_data, train_labels)\n    d_trees.append(clf)\n    \n    # make its predictions on test data\n    pred \u003d d_trees[i].predict(subset_test_data)\n    \n    # find auc scores\n    auc \u003d roc_auc_score(test_labels, pred)\n    \n    # record the scores\n    dt_aucs.append(auc)\n\n\nprint(\u0027Decision Tree Results:\u0027)\nprint(\u0027   \\tAUC\\tAcc (%dit) (mean ± std)\\t\\tFeatures\u0027 % NUM_ITER)\nfor i, f in enumerate(zip(dt_aucs, dt_cas, dt_cas_std)):\n    print(\u0027%1d\u0027 %i,\u0027\\t%05.3f\\t%05.3f ± %05.03f\\t\u0027 % tuple(f) + \u0027, \u0027.join((best_features[i])))\n\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      },
      "source": [
        "## AUC and Classification Accuracy - Random Forest Walk\n",
        "\n",
        "The code below will find the classification accuracy using 10-fold cross-validation using stratified sampling to help class imbalance. The AUC on the test split is also found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": "# visualize random forest features\nrfws \u003d []\n\n\n# find CA - uses 10-fold cross validation \n# with stratified sampling to help with class imbalance\n# and simple average over subsets\nrfw_cas \u003d []\n\n# maintain list of cas over a period\nrfw_ca_matrix \u003d []\n\n# run the thing NUM_ITER times\nfor _ in range(NUM_ITER):\n    for i in range(k):\n        aucs \u003d []\n        # make fold\n        skf \u003d StratifiedKFold(n_splits\u003d10, shuffle\u003dTrue)\n        for trx, tex in skf.split(all_data, all_labels):\n            # strip data to required features\n            subset_data \u003d all_data.filter(best_features[i], axis\u003d1)\n            \n            # find auc\n            rfwtree \u003d RandomForestClassifier(n_estimators\u003d100)\n            rfwtree.fit(subset_data.iloc[trx, :], all_labels.iloc[trx])\n            pred \u003d rfwtree.predict(subset_data.iloc[tex, :])\n            labels \u003d all_labels.iloc[tex]\n            \n            acc \u003d roc_auc_score(labels, pred)\n            # record auc to average later\n            aucs.append(acc)\n        \n        rfw_cas.append(np.mean(aucs))\n    rfw_ca_matrix.append(list(rfw_cas))\n    rfw_cas.clear()\n\n\nrfw_ca_matrix \u003d np.array(rfw_ca_matrix)\nrfw_cas \u003d rfw_ca_matrix.mean(axis\u003d0)\nrfw_cas_std \u003d rfw_ca_matrix.std(axis\u003d0)\n\n# find AUC \nrfw_aucs \u003d []\nfor i in range(k):\n    subset_test_data \u003d test_data.filter(best_features[i], axis\u003d1)\n    subset_train_data \u003d train_data.filter(best_features[i], axis\u003d1)\n    \n    clf \u003d RandomForestClassifier(n_estimators\u003d100)\n    clf.fit(subset_train_data, train_labels)\n    rfws.append(clf)\n    \n    # make its predictions on test data\n    pred \u003d rfws[i].predict(subset_test_data)\n    \n    # find auc scores\n    auc \u003d roc_auc_score(test_labels, pred)\n    \n    # record the scores\n    rfw_aucs.append(auc)\n\n\nprint(\u0027Random Forest Results:\u0027)\nprint(\u0027   \\tAUC\\tAcc (%dit) (mean ± std)\\t\\tFeatures\u0027 % NUM_ITER)\nfor i, f in enumerate(zip(dt_aucs, rfw_cas, rfw_cas_std)):\n    print(\u0027%1d\u0027 % i, \u0027\\t%05.3f\\t%05.3f ± %05.03f\\t\u0027 % tuple(f) + \u0027, \u0027.join(sorted(best_features[i])))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        " \n",
        "## Tabulate Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": "to_write \u003d [[\u0027AUC DT\u0027,\u0027AUC RFW\u0027,\u0027Acc DT\u0027, \u0027Std Acc DT\u0027 ,\u0027Acc RFW\u0027, \u0027Std Acc RFW\u0027, \u0027Features\u0027]]\n\nprint(\u0027   AUC(DT)\\tAUC(RFW)Acc(DT %dit, x± std)\\tAcc(RFW %dit, x±std)\\tFeatures\u0027 % (NUM_ITER, \n                                                                                                     NUM_ITER))\nfor i, f in enumerate(zip(dt_aucs, rfw_aucs, dt_cas, dt_cas_std, rfw_cas, rfw_cas_std)):\n    print(\u0027%1d\u0027 % i, \u0027%05.3f\\t%05.3f\\t%05.3f ± %05.03f\\t%05.3f ± %05.03f\\t\u0027 % tuple(f) + \u0027, \u0027.join((best_features[i])))\n    to_write.append(list(f) + [\u0027, \u0027.join((best_features[i]))])\n    \n# write results to a csv file\noutput \u003d csv.writer(open(\u0027Final Result.csv\u0027, \u0027w\u0027, newline\u003d\u0027\u0027))\noutput.writerows(to_write)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "print(k)\nprint(map(len, [dt_aucs, rfw_aucs, dt_cas, dt_cas_std, rfw_cas, rfw_cas_std]))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Feature importance\n\nThe feature importances are compared below for decision trees and random forests.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "i \u003d 0\n# visualization\nfor dtree in d_trees:\n    if i \u003c k:\n        print(\u0027Feature importances for tree and forest (resp.) %s/%s:\u0027 % (i + 1, k))\n        for e in zip(dtree.feature_importances_, rfws[i].feature_importances_, best_features[i]):\n            print(\u0027\\t%6f\\t%6f\\t%s\u0027 % e)\n\n    else:\n        print(\u0027Warning, code may be buggy\u0027)\n    i +\u003d 1\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}