{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data  = pd.read_csv('results7parsed.csv')\n",
    "\n",
    "# sort by best CA\n",
    "avg_ca = data.sort_values('Average CA', ascending=False)\n",
    "avg_rfw = data.sort_values('RFW CA', ascending=False)\n",
    "avg_dt = data.sort_values('DT CA', ascending=False)\n",
    "\n",
    "# get best 5 average features\n",
    "with open('CA Features.csv', 'w')as f :\n",
    "    f.write('Avg')\n",
    "    f.write(avg_ca[:5].to_csv())\n",
    "    \n",
    "# get best 5 random forest features\n",
    "with open('CA Features.csv', 'a')as f :\n",
    "    f.write('Rfw')\n",
    "    f.write(avg_rfw[:5].to_csv())\n",
    "    \n",
    "# get 5 best decision tree features \n",
    "with open('CA Features.csv', 'a')as f :\n",
    "    f.write('Dtree')\n",
    "    f.write(avg_dt[:5].to_csv())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let us take the most interesting features from the csv and write a parser to analyze the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_negCharge, PSA/Area'], ['nof_SO3H, posCharge/Volume, C_R1, nof_posCharge, nof_HBA, PSA/Area, molLogS'], ['nof_OH, nof_SO3H, nof_negCharge, nof_posCharge, PSA/Area, molPSA, molLogP'], ['nof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_posCharge, PSA/Area'], ['nof_OH, nof_COOH, nof_NH2, nof_PO4, C_R2, nof_HBA, PSA/Area'], ['nof_OH, nof_NH2, nof_SO3H, C_R0, nof_HBA, PSA/Area, molLogS'], ['nof_OH, posCharge/Volume, C_R0, nof_HBA, PSA/Area, molLogS, molLogP'], ['nof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_HBA, PSA/Area'], ['nof_OH, nof_NH2, nof_PO4, C_R0, nof_posCharge, nof_HBA, PSA/Area'], ['nof_OH, nof_NH2, nof_SO3H, negCharge/Volume, nof_HBA, PSA/Area, molLogS'], ['PSA/Area, nof_Rings, Complexity, nof_SO3H, nof_OH, nof_Chirals, C_R0']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s ='''nof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_negCharge, PSA/Area\n",
    "nof_SO3H, posCharge/Volume, C_R1, nof_posCharge, nof_HBA, PSA/Area, molLogS\n",
    "nof_OH, nof_SO3H, nof_negCharge, nof_posCharge, PSA/Area, molPSA, molLogP\n",
    "nof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_posCharge, PSA/Area\n",
    "nof_OH, nof_COOH, nof_NH2, nof_PO4, C_R2, nof_HBA, PSA/Area\n",
    "nof_OH, nof_NH2, nof_SO3H, C_R0, nof_HBA, PSA/Area, molLogS\n",
    "nof_OH, posCharge/Volume, C_R0, nof_HBA, PSA/Area, molLogS, molLogP\n",
    "nof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_HBA, PSA/Area\n",
    "nof_OH, nof_NH2, nof_PO4, C_R0, nof_posCharge, nof_HBA, PSA/Area\n",
    "nof_OH, nof_NH2, nof_SO3H, negCharge/Volume, nof_HBA, PSA/Area, molLogS\n",
    "PSA/Area, nof_Rings, Complexity, nof_SO3H, nof_OH, nof_Chirals, C_R0'''\n",
    "\n",
    "features =  [sorted([x]) for x in s.split('\\n')]\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Project Settings\n",
    "\n",
    "Specified here are the paths for the data and the features to run over in the list of best features.\n",
    "Each entry in the list is a list containing one single string of the features to try, comma seperated. In this way it is easy to write a script to \n",
    "add entries to try very easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##### set hyperparams\n",
    "NUM_ITER = 10 # number of times to run 10foldxval to get a statistical degree of confidence\n",
    "\n",
    "''' HYPERPARAMS FOR DECISION TREE\n",
    " \n",
    " These parameters implement a rudimentary pruning algorithm, would ideally like to use AB pruning'''\n",
    "enable_pruning = True\n",
    "# maximum depth of dtree\n",
    "max_depth = 5\n",
    "# how many samples your need atleast, at a LEAF node\n",
    "min_samples = 3\n",
    "\n",
    "##### set parameters\n",
    "path_train_data = 'train.csv'\n",
    "path_test_data = 'test.csv'\n",
    "path_all_data = 'Dataset Correlated Removed.csv'\n",
    "\n",
    "# set features here\n",
    "\n",
    "best_features = features\n",
    "\n",
    "best_features = [list(map(str.strip, x[0].split(','))) for x in best_features]\n",
    "\n",
    "k = len(best_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "This code loads dataset into the variables below and converts the labels to categorical 0, 1 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "all_data = pd.DataFrame(pd.read_csv(path_all_data))\n",
    "all_labels = all_data['SLC'].astype('category').cat.codes\n",
    "# drop labels\n",
    "all_data.drop('SLC', axis=1, inplace=True)\n",
    "\n",
    "train_data = pd.DataFrame(pd.read_csv(path_train_data))\n",
    "train_labels = train_data['SLC'].astype('category').cat.codes\n",
    "# drop labels\n",
    "\n",
    "train_data.drop('SLC', axis=1, inplace=True)\n",
    "\n",
    "test_data = pd.DataFrame(pd.read_csv(path_test_data))\n",
    "test_labels = test_data['SLC'].astype('category').cat.codes\n",
    "# drop labels\n",
    "test_data.drop('SLC', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## AUC and Classification Accuracy - Decision Tree\n",
    "\n",
    "The code below will find the classification accuracy using 10-fold cross-validation using stratified sampling to help class imbalance. The AUC on the test split is also found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results:\n",
      "   \tAUC\tAcc (10it) (mean ± std)\t\tFeatures\n",
      "0 \t0.667\t0.725 ± 0.012\tnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_negCharge, PSA/Area\n",
      "1 \t0.714\t0.671 ± 0.028\tnof_SO3H, posCharge/Volume, C_R1, nof_posCharge, nof_HBA, PSA/Area, molLogS\n",
      "2 \t0.726\t0.657 ± 0.024\tnof_OH, nof_SO3H, nof_negCharge, nof_posCharge, PSA/Area, molPSA, molLogP\n",
      "3 \t0.667\t0.729 ± 0.020\tnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_posCharge, PSA/Area\n",
      "4 \t0.690\t0.721 ± 0.006\tnof_OH, nof_COOH, nof_NH2, nof_PO4, C_R2, nof_HBA, PSA/Area\n",
      "5 \t0.679\t0.698 ± 0.029\tnof_OH, nof_NH2, nof_SO3H, C_R0, nof_HBA, PSA/Area, molLogS\n",
      "6 \t0.690\t0.690 ± 0.027\tnof_OH, posCharge/Volume, C_R0, nof_HBA, PSA/Area, molLogS, molLogP\n",
      "7 \t0.667\t0.717 ± 0.024\tnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_HBA, PSA/Area\n",
      "8 \t0.726\t0.691 ± 0.032\tnof_OH, nof_NH2, nof_PO4, C_R0, nof_posCharge, nof_HBA, PSA/Area\n",
      "9 \t0.679\t0.694 ± 0.027\tnof_OH, nof_NH2, nof_SO3H, negCharge/Volume, nof_HBA, PSA/Area, molLogS\n",
      "10 \t0.679\t0.688 ± 0.015\tPSA/Area, nof_Rings, Complexity, nof_SO3H, nof_OH, nof_Chirals, C_R0\n"
     ]
    }
   ],
   "source": [
    "# visualize decision tree for input features\n",
    "\n",
    "d_trees = []\n",
    "\n",
    "\n",
    "# find CA - uses 10-fold cross validation \n",
    "# with stratified sampling to help with class imbalance\n",
    "# and simple average over subsets\n",
    "dt_cas = []\n",
    "\n",
    "# maintain list of cas over a period\n",
    "dt_ca_matrix = []\n",
    "\n",
    "# run the thing NUM_ITER times\n",
    "for _ in range(NUM_ITER):\n",
    "    for i in range(k):\n",
    "        aucs = []\n",
    "        # make fold\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "        for trx, tex in skf.split(all_data, all_labels):\n",
    "            # strip data to required features\n",
    "            subset_data = all_data.filter(best_features[i], axis=1)\n",
    "            \n",
    "            # find auc\n",
    "            dtree = DecisionTreeClassifier(presort=True, max_depth=max_depth, min_samples_leaf=min_samples)\n",
    "            dtree.fit(subset_data.iloc[trx, :], all_labels.iloc[trx])        \n",
    "            pred = dtree.predict(subset_data.iloc[tex, :])\n",
    "            labels = all_labels.iloc[tex]\n",
    "            \n",
    "            acc = roc_auc_score(labels, pred)\n",
    "            # record auc to average later\n",
    "            aucs.append(acc)\n",
    "        \n",
    "        dt_cas.append(np.mean(aucs))\n",
    "        \n",
    "    dt_ca_matrix.append(list(dt_cas))\n",
    "    dt_cas.clear()\n",
    "\n",
    "\n",
    "dt_ca_matrix = np.array(dt_ca_matrix)\n",
    "dt_cas = dt_ca_matrix.mean(axis=0)\n",
    "dt_cas_std = dt_ca_matrix.std(axis=0)\n",
    "\n",
    "# find AUC \n",
    "dt_aucs = []\n",
    "\n",
    "# run k-fold validation\n",
    "for i in range(k):\n",
    "    subset_test_data = test_data.filter(best_features[i], axis=1)\n",
    "    subset_train_data = train_data.filter(best_features[i], axis=1)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(presort=True, max_depth=max_depth, min_samples_leaf=min_samples)\n",
    "    clf.fit(subset_train_data, train_labels)\n",
    "    d_trees.append(clf)\n",
    "    \n",
    "    # make its predictions on test data\n",
    "    pred = d_trees[i].predict(subset_test_data)\n",
    "    \n",
    "    # find auc scores\n",
    "    auc = roc_auc_score(test_labels, pred)\n",
    "    \n",
    "    # record the scores\n",
    "    dt_aucs.append(auc)\n",
    "\n",
    "\n",
    "print('Decision Tree Results:')\n",
    "print('   \\tAUC\\tAcc (%dit) (mean ± std)\\t\\tFeatures' % NUM_ITER)\n",
    "for i, f in enumerate(zip(dt_aucs, dt_cas, dt_cas_std)):\n",
    "    print('%1d' %i,'\\t%05.3f\\t%05.3f ± %05.03f\\t' % tuple(f) + ', '.join((best_features[i])))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## AUC and Classification Accuracy - Random Forest Walk\n",
    "\n",
    "The code below will find the classification accuracy using 10-fold cross-validation using stratified sampling to help class imbalance. The AUC on the test split is also found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "   \tAUC\tAcc (10it) (mean ± std)\t\tFeatures\n",
      "0 \t0.667\t0.747 ± 0.019\tC_R2, C_sp3, PSA/Area, nof_NH2, nof_OH, nof_SO3H, nof_negCharge\n",
      "1 \t0.714\t0.769 ± 0.015\tC_R1, PSA/Area, molLogS, nof_HBA, nof_SO3H, nof_posCharge, posCharge/Volume\n",
      "2 \t0.726\t0.734 ± 0.013\tPSA/Area, molLogP, molPSA, nof_OH, nof_SO3H, nof_negCharge, nof_posCharge\n",
      "3 \t0.667\t0.742 ± 0.015\tC_R2, C_sp3, PSA/Area, nof_NH2, nof_OH, nof_SO3H, nof_posCharge\n",
      "4 \t0.690\t0.742 ± 0.015\tC_R2, PSA/Area, nof_COOH, nof_HBA, nof_NH2, nof_OH, nof_PO4\n",
      "5 \t0.679\t0.789 ± 0.009\tC_R0, PSA/Area, molLogS, nof_HBA, nof_NH2, nof_OH, nof_SO3H\n",
      "6 \t0.690\t0.760 ± 0.016\tC_R0, PSA/Area, molLogP, molLogS, nof_HBA, nof_OH, posCharge/Volume\n",
      "7 \t0.667\t0.778 ± 0.018\tC_R2, C_sp3, PSA/Area, nof_HBA, nof_NH2, nof_OH, nof_SO3H\n",
      "8 \t0.726\t0.789 ± 0.007\tC_R0, PSA/Area, nof_HBA, nof_NH2, nof_OH, nof_PO4, nof_posCharge\n",
      "9 \t0.679\t0.767 ± 0.023\tPSA/Area, molLogS, negCharge/Volume, nof_HBA, nof_NH2, nof_OH, nof_SO3H\n",
      "10 \t0.679\t0.743 ± 0.018\tC_R0, Complexity, PSA/Area, nof_Chirals, nof_OH, nof_Rings, nof_SO3H\n"
     ]
    }
   ],
   "source": [
    "# visualize random forest features\n",
    "rfws = []\n",
    "\n",
    "\n",
    "# find CA - uses 10-fold cross validation \n",
    "# with stratified sampling to help with class imbalance\n",
    "# and simple average over subsets\n",
    "rfw_cas = []\n",
    "\n",
    "# maintain list of cas over a period\n",
    "rfw_ca_matrix = []\n",
    "\n",
    "# run the thing NUM_ITER times\n",
    "for _ in range(NUM_ITER):\n",
    "    for i in range(k):\n",
    "        aucs = []\n",
    "        # make fold\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "        for trx, tex in skf.split(all_data, all_labels):\n",
    "            # strip data to required features\n",
    "            subset_data = all_data.filter(best_features[i], axis=1)\n",
    "            \n",
    "            # find auc\n",
    "            rfwtree = RandomForestClassifier(n_estimators=100)\n",
    "            rfwtree.fit(subset_data.iloc[trx, :], all_labels.iloc[trx])\n",
    "            pred = rfwtree.predict(subset_data.iloc[tex, :])\n",
    "            labels = all_labels.iloc[tex]\n",
    "            \n",
    "            acc = roc_auc_score(labels, pred)\n",
    "            # record auc to average later\n",
    "            aucs.append(acc)\n",
    "        \n",
    "        rfw_cas.append(np.mean(aucs))\n",
    "    rfw_ca_matrix.append(list(rfw_cas))\n",
    "    rfw_cas.clear()\n",
    "\n",
    "\n",
    "rfw_ca_matrix = np.array(rfw_ca_matrix)\n",
    "rfw_cas = rfw_ca_matrix.mean(axis=0)\n",
    "rfw_cas_std = rfw_ca_matrix.std(axis=0)\n",
    "\n",
    "# find AUC \n",
    "rfw_aucs = []\n",
    "for i in range(k):\n",
    "    subset_test_data = test_data.filter(best_features[i], axis=1)\n",
    "    subset_train_data = train_data.filter(best_features[i], axis=1)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(subset_train_data, train_labels)\n",
    "    rfws.append(clf)\n",
    "    \n",
    "    # make its predictions on test data\n",
    "    pred = rfws[i].predict(subset_test_data)\n",
    "    \n",
    "    # find auc scores\n",
    "    auc = roc_auc_score(test_labels, pred)\n",
    "    \n",
    "    # record the scores\n",
    "    rfw_aucs.append(auc)\n",
    "\n",
    "\n",
    "print('Random Forest Results:')\n",
    "print('   \\tAUC\\tAcc (%dit) (mean ± std)\\t\\tFeatures' % NUM_ITER)\n",
    "for i, f in enumerate(zip(dt_aucs, rfw_cas, rfw_cas_std)):\n",
    "    print('%1d' % i, '\\t%05.3f\\t%05.3f ± %05.03f\\t' % tuple(f) + ', '.join(sorted(best_features[i])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    " \n",
    "## Tabulate Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AUC(DT)\tAUC(RFW)Acc(DT 10it, x± std)\tAcc(RFW 10it, x±std)\tFeatures\n",
      "0 0.667\t0.786\t0.725 ± 0.012\t0.747 ± 0.019\tnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_negCharge, PSA/Area\n",
      "1 0.714\t0.786\t0.671 ± 0.028\t0.769 ± 0.015\tnof_SO3H, posCharge/Volume, C_R1, nof_posCharge, nof_HBA, PSA/Area, molLogS\n",
      "2 0.726\t0.738\t0.657 ± 0.024\t0.734 ± 0.013\tnof_OH, nof_SO3H, nof_negCharge, nof_posCharge, PSA/Area, molPSA, molLogP\n",
      "3 0.667\t0.762\t0.729 ± 0.020\t0.742 ± 0.015\tnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_posCharge, PSA/Area\n",
      "4 0.690\t0.762\t0.721 ± 0.006\t0.742 ± 0.015\tnof_OH, nof_COOH, nof_NH2, nof_PO4, C_R2, nof_HBA, PSA/Area\n",
      "5 0.679\t0.762\t0.698 ± 0.029\t0.789 ± 0.009\tnof_OH, nof_NH2, nof_SO3H, C_R0, nof_HBA, PSA/Area, molLogS\n",
      "6 0.690\t0.774\t0.690 ± 0.027\t0.760 ± 0.016\tnof_OH, posCharge/Volume, C_R0, nof_HBA, PSA/Area, molLogS, molLogP\n",
      "7 0.667\t0.786\t0.717 ± 0.024\t0.778 ± 0.018\tnof_OH, nof_NH2, nof_SO3H, C_sp3, C_R2, nof_HBA, PSA/Area\n",
      "8 0.726\t0.738\t0.691 ± 0.032\t0.789 ± 0.007\tnof_OH, nof_NH2, nof_PO4, C_R0, nof_posCharge, nof_HBA, PSA/Area\n",
      "9 0.679\t0.750\t0.694 ± 0.027\t0.767 ± 0.023\tnof_OH, nof_NH2, nof_SO3H, negCharge/Volume, nof_HBA, PSA/Area, molLogS\n",
      "10 0.679\t0.774\t0.688 ± 0.015\t0.743 ± 0.018\tPSA/Area, nof_Rings, Complexity, nof_SO3H, nof_OH, nof_Chirals, C_R0\n"
     ]
    }
   ],
   "source": [
    "to_write = [['AUC DT','AUC RFW','Acc DT', 'Std Acc DT' ,'Acc RFW', 'Std Acc RFW', 'Features']]\n",
    "\n",
    "print('   AUC(DT)\\tAUC(RFW)Acc(DT %dit, x± std)\\tAcc(RFW %dit, x±std)\\tFeatures' % (NUM_ITER, \n",
    "                                                                                                     NUM_ITER))\n",
    "for i, f in enumerate(zip(dt_aucs, rfw_aucs, dt_cas, dt_cas_std, rfw_cas, rfw_cas_std)):\n",
    "    print('%1d' % i, '%05.3f\\t%05.3f\\t%05.3f ± %05.03f\\t%05.3f ± %05.03f\\t' % tuple(f) + ', '.join((best_features[i])))\n",
    "    to_write.append(list(f) + [', '.join((best_features[i]))])\n",
    "    \n",
    "# write results to a csv file\n",
    "output = csv.writer(open('Final Result.csv', 'w', newline=''))\n",
    "output.writerows(to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "<map object at 0x0000029D7F7BDD68>\n"
     ]
    }
   ],
   "source": [
    "print(k)\n",
    "print(map(len, [dt_aucs, rfw_aucs, dt_cas, dt_cas_std, rfw_cas, rfw_cas_std]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Feature importance\n",
    "\n",
    "The feature importances are compared below for decision trees and random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances for tree and forest (resp.) 1/11:\n",
      "\t0.284552\t0.128307\tnof_OH\n",
      "\t0.030408\t0.059543\tnof_NH2\n",
      "\t0.000000\t0.024704\tnof_SO3H\n",
      "\t0.082834\t0.206792\tC_sp3\n",
      "\t0.000000\t0.047691\tC_R2\n",
      "\t0.028885\t0.080044\tnof_negCharge\n",
      "\t0.573321\t0.452918\tPSA/Area\n",
      "Feature importances for tree and forest (resp.) 2/11:\n",
      "\t0.000000\t0.021442\tnof_SO3H\n",
      "\t0.000000\t0.076787\tposCharge/Volume\n",
      "\t0.271126\t0.115341\tC_R1\n",
      "\t0.000000\t0.028120\tnof_posCharge\n",
      "\t0.041304\t0.130628\tnof_HBA\n",
      "\t0.687570\t0.380611\tPSA/Area\n",
      "\t0.000000\t0.247071\tmolLogS\n",
      "Feature importances for tree and forest (resp.) 3/11:\n",
      "\t0.275193\t0.116043\tnof_OH\n",
      "\t0.000000\t0.020122\tnof_SO3H\n",
      "\t0.027935\t0.064275\tnof_negCharge\n",
      "\t0.000000\t0.050351\tnof_posCharge\n",
      "\t0.488481\t0.334787\tPSA/Area\n",
      "\t0.101437\t0.187940\tmolPSA\n",
      "\t0.106953\t0.226483\tmolLogP\n",
      "Feature importances for tree and forest (resp.) 4/11:\n",
      "\t0.296321\t0.150659\tnof_OH\n",
      "\t0.031665\t0.043493\tnof_NH2\n",
      "\t0.000000\t0.019974\tnof_SO3H\n",
      "\t0.061232\t0.202083\tC_sp3\n",
      "\t0.000000\t0.041363\tC_R2\n",
      "\t0.000000\t0.045192\tnof_posCharge\n",
      "\t0.610782\t0.497236\tPSA/Area\n",
      "Feature importances for tree and forest (resp.) 5/11:\n",
      "\t0.279829\t0.152869\tnof_OH\n",
      "\t0.056726\t0.061008\tnof_COOH\n",
      "\t0.029903\t0.058059\tnof_NH2\n",
      "\t0.000000\t0.006472\tnof_PO4\n",
      "\t0.000000\t0.047236\tC_R2\n",
      "\t0.082188\t0.176760\tnof_HBA\n",
      "\t0.551354\t0.497596\tPSA/Area\n",
      "Feature importances for tree and forest (resp.) 6/11:\n",
      "\t0.297253\t0.139071\tnof_OH\n",
      "\t0.031765\t0.048979\tnof_NH2\n",
      "\t0.000000\t0.017238\tnof_SO3H\n",
      "\t0.050918\t0.156404\tC_R0\n",
      "\t0.033617\t0.135601\tnof_HBA\n",
      "\t0.543202\t0.330791\tPSA/Area\n",
      "\t0.043245\t0.171916\tmolLogS\n",
      "Feature importances for tree and forest (resp.) 7/11:\n",
      "\t0.271951\t0.129470\tnof_OH\n",
      "\t0.112686\t0.069375\tposCharge/Volume\n",
      "\t0.098509\t0.130656\tC_R0\n",
      "\t0.000000\t0.086284\tnof_HBA\n",
      "\t0.456984\t0.242063\tPSA/Area\n",
      "\t0.046584\t0.137032\tmolLogS\n",
      "\t0.013285\t0.205120\tmolLogP\n",
      "Feature importances for tree and forest (resp.) 8/11:\n",
      "\t0.301795\t0.143607\tnof_OH\n",
      "\t0.032250\t0.049364\tnof_NH2\n",
      "\t0.000000\t0.020291\tnof_SO3H\n",
      "\t0.041497\t0.189342\tC_sp3\n",
      "\t0.000000\t0.030005\tC_R2\n",
      "\t0.034131\t0.166594\tnof_HBA\n",
      "\t0.590327\t0.400797\tPSA/Area\n",
      "Feature importances for tree and forest (resp.) 9/11:\n",
      "\t0.295570\t0.150299\tnof_OH\n",
      "\t0.031585\t0.042383\tnof_NH2\n",
      "\t0.000000\t0.006547\tnof_PO4\n",
      "\t0.061268\t0.204613\tC_R0\n",
      "\t0.000000\t0.044374\tnof_posCharge\n",
      "\t0.033427\t0.147324\tnof_HBA\n",
      "\t0.578150\t0.404459\tPSA/Area\n",
      "Feature importances for tree and forest (resp.) 10/11:\n",
      "\t0.277027\t0.133231\tnof_OH\n",
      "\t0.029604\t0.036612\tnof_NH2\n",
      "\t0.000000\t0.023129\tnof_SO3H\n",
      "\t0.124491\t0.209818\tnegCharge/Volume\n",
      "\t0.031330\t0.117585\tnof_HBA\n",
      "\t0.490095\t0.292385\tPSA/Area\n",
      "\t0.047454\t0.187240\tmolLogS\n",
      "Feature importances for tree and forest (resp.) 11/11:\n",
      "\t0.571670\t0.305501\tPSA/Area\n",
      "\t0.000000\t0.058083\tnof_Rings\n",
      "\t0.014641\t0.223913\tComplexity\n",
      "\t0.000000\t0.017899\tnof_SO3H\n",
      "\t0.000000\t0.092376\tnof_OH\n",
      "\t0.350710\t0.130425\tnof_Chirals\n",
      "\t0.062979\t0.171803\tC_R0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "# visualization\n",
    "for dtree in d_trees:\n",
    "    if i < k:\n",
    "        print('Feature importances for tree and forest (resp.) %s/%s:' % (i + 1, k))\n",
    "        for e in zip(dtree.feature_importances_, rfws[i].feature_importances_, best_features[i]):\n",
    "            print('\\t%6f\\t%6f\\t%s' % e)\n",
    "\n",
    "    else:\n",
    "        print('Warning, code may be buggy')\n",
    "    i += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
